{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b05894-4ca9-4864-8aac-7a92727e13c5",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abdfce4-f5e8-4aa6-89b5-b3018944a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4645e9fe-ebb5-4896-abb8-bdf461aa68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(engine, query):\n",
    "    results = None\n",
    "    with engine.connect() as conn:\n",
    "        results = pd.DataFrame(conn.execute(text(query)).fetchall())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8dfe6-c570-4dc0-8980-bd5040d3ae86",
   "metadata": {},
   "source": [
    "### Load the data using SQL\n",
    "We will load the data as if it was stored in a database. This helps us to simulate the data extraction phase that usually happens in a commercial environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d155e8-66c0-4ddb-95a2-ee51e25704a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6822934"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pickle file into a dataframe\n",
    "df = pd.read_pickle(\"./data/customer_product.pkl\")\n",
    "\n",
    "# Fix column name and age values\n",
    "df = df.rename(columns={\"travel_insurace\": \"travel_insurance\"})\n",
    "df[\"age\"] = df[\"age\"].map(lambda x: float(x) if x != ' NA' else None)\n",
    "\n",
    "# Sort by customer_id and date_partition\n",
    "df = df.sort_values(by=['customer_id', 'date_partition'])\n",
    "\n",
    "# Apply shift to get the previous date_partition on which the same customer was updated\n",
    "# We will use that to join the target variable with the features (which require to be 1 month lagged)\n",
    "df['next_date_partition'] = df.groupby('customer_id')['date_partition'].shift(-1)\n",
    "\n",
    "# Load data into a table\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "df.to_sql(name='customer_product', con=engine, if_exists=\"replace\", chunksize=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68327603-415b-4118-af3f-04b21462dbfd",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "We will generate the Purchase table. This will contain all purchases on customer and date level. We will use it to create our X,y pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62d6f94-6d36-43bb-b364-6c4cba8bcb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for investment_account...completed [n=53]\n",
      "Running for disability_insurance...completed [n=13]\n",
      "Running for retail...completed [n=370803]\n",
      "Running for accounting_link...completed [n=269]\n",
      "Running for company_insurance...completed [n=60575]\n",
      "Running for accounting_package...completed [n=5237]\n",
      "Running for pension...completed [n=103671]\n",
      "Running for credit_card...completed [n=87943]\n",
      "Running for travel_insurance...completed [n=23870]\n",
      "Running for job_insurance...completed [n=14491]\n",
      "Running for legal_insurance...completed [n=2754]\n",
      "Running for accident_insurance...completed [n=49260]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define product columns\n",
    "product_cols = [\n",
    "    \"investment_account\",\n",
    "    \"disability_insurance\",\n",
    "    \"retail\",\n",
    "    \"accounting_link\",\n",
    "    \"company_insurance\",\n",
    "    \"accounting_package\",\n",
    "    \"pension\",\n",
    "    \"credit_card\",\n",
    "    \"travel_insurance\",\n",
    "    \"job_insurance\",\n",
    "    \"legal_insurance\",\n",
    "    \"accident_insurance\",\n",
    "]\n",
    "\n",
    "# For each product we will generate a table that contains all purchases.\n",
    "# We will perform this for all reoccurances of such events per customer.\n",
    "results_df_purchases = pd.DataFrame()\n",
    "results_df_churns = pd.DataFrame()\n",
    "for product in product_cols:\n",
    "    sql = f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        1.0 - purchased AS churned\n",
    "    FROM\n",
    "    (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            date_partition,\n",
    "            product_cd,\n",
    "            CASE\n",
    "                WHEN curr_status > prev_status THEN 1.0\n",
    "                ELSE 0.0\n",
    "            END as purchased\n",
    "        FROM\n",
    "        (\n",
    "            SELECT \n",
    "                customer_id,\n",
    "                date_partition,\n",
    "                '{product}' as product_cd,\n",
    "                LAG({product}, 1, 0) OVER (PARTITION BY customer_id ORDER BY date_partition ASC) as prev_status, \n",
    "                {product} as curr_status\n",
    "            FROM customer_product\n",
    "        )\n",
    "        where prev_status != curr_status\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(f\"Running for '{product}'...\", end=\"\")\n",
    "    result = run_query(engine, sql)\n",
    "    print(f\" [DONE] (n={len(result)})\")\n",
    "    \n",
    "    # Save results to tables\n",
    "    results_df_purchases = pd.concat([results_df_purchases, result[result[\"purchased\"]==1.0]], axis=0)\n",
    "    results_df_churns = pd.concat([results_df_churns, result[result[\"churned\"]==1.0]], axis=0)\n",
    "\n",
    "results_df_purchases.to_sql(name='product_purchases', con=engine, if_exists=\"replace\")\n",
    "results_df_churns.to_sql(name='product_churns', con=engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "55f49d81-f879-4bc0-bd8b-8f2198fc7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\n",
    "    \"retail\", \"company_insurance\", \"accounting_package\", \"pension\", \"credit_card\", \n",
    "    \"travel_insurance\", \"job_insurance\", \"legal_insurance\", \"accident_insurance\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0383ee-4669-46fa-bf32-152253394096",
   "metadata": {},
   "source": [
    "### Create the target and some purchase related features\n",
    "We will join our dataset with the target variable and will create some purchase related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "773ed943-1ac0-4969-b659-af09e3471d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for 'retail'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.8615799406848207\n",
      "\tPrecision: 0.799939921898468\n",
      "\tRecall: 0.9335125029212433\n",
      "\tAUC: 0.9528296443708951\n",
      "Running for 'company_insurance'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.8384722498508056\n",
      "\tPrecision: 0.8101095521814338\n",
      "\tRecall: 0.8688930117501547\n",
      "\tAUC: 0.9324613081130698\n",
      "Running for 'accounting_package'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.9929078014184397\n",
      "\tPrecision: 0.9859154929577465\n",
      "\tRecall: 1.0\n",
      "\tAUC: 1.0\n",
      "Running for 'pension'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.8836483155299918\n",
      "\tPrecision: 0.8796728016359918\n",
      "\tRecall: 0.8876599257119273\n",
      "\tAUC: 0.9529156196413512\n",
      "Running for 'credit_card'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.8447440488729724\n",
      "\tPrecision: 0.7617057650299174\n",
      "\tRecall: 0.9481026126019624\n",
      "\tAUC: 0.9522801522923988\n",
      "Running for 'travel_insurance'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.7186557518311073\n",
      "\tPrecision: 0.6742118027485853\n",
      "\tRecall: 0.7693726937269373\n",
      "\tAUC: 0.8340490396551431\n",
      "Running for 'job_insurance'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.7740940632228219\n",
      "\tPrecision: 0.6820652173913043\n",
      "\tRecall: 0.8948306595365418\n",
      "\tAUC: 0.8911105090814859\n",
      "Running for 'legal_insurance'...\n",
      "\tBuilding dataset...\n",
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.8571428571428571\n",
      "\tPrecision: 0.75\n",
      "\tRecall: 1.0\n",
      "\tAUC: 0.9365079365079365\n",
      "Running for 'accident_insurance'...\n",
      "\tBuilding dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['days_since_last_purchase']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['days_since_last_purchase']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPreparing X,y...\n",
      "\tTraining model...\n",
      "\tEvaluation on test set...\n",
      "\tF1: 0.8288782816229117\n",
      "\tPrecision: 0.8063617367076852\n",
      "\tRecall: 0.8526884360422293\n",
      "\tAUC: 0.9078991330349117\n"
     ]
    }
   ],
   "source": [
    "# Define the features\n",
    "num_features = [\"age\", \"customer_seniority\", \"gross_income\", \"times_purchased\", \"days_since_last_purchase\"]\n",
    "cat_features = [\"sex\", \"new_customer\", \"acquisition_channel\", \"activity_status\", \"education_segment\", \"sbi\", \"date_month\"]\n",
    "cat_features = cat_features + product_cols\n",
    "features = cat_features + num_features\n",
    "\n",
    "# Define the target variable\n",
    "target = \"target\"\n",
    "\n",
    "feature_importances = {}\n",
    "\n",
    "for product in products:\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        date_partition,\n",
    "        times_purchased,\n",
    "        julianday(date_partition) - julianday(last_purchase) as days_since_last_purchase,\n",
    "        strftime('%m', date_partition) as date_month,\n",
    "        target\n",
    "    FROM\n",
    "    (\n",
    "        SELECT\n",
    "            customer_id,\n",
    "            date_partition,\n",
    "            coalesce(sum(purchased) over (partition by customer_id order by date_partition asc rows between unbounded preceding and 1 preceding),0) as times_purchased,\n",
    "            purchased as target,\n",
    "            MAX(purchase_dt) OVER (\n",
    "                PARTITION BY customer_id\n",
    "                ORDER BY date_partition ASC\n",
    "                ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n",
    "            ) AS last_purchase\n",
    "        FROM\n",
    "        (\n",
    "            SELECT \n",
    "                a.*,\n",
    "                coalesce(b.purchased, 0) as purchased,\n",
    "                purchase_dt\n",
    "            FROM customer_product a\n",
    "            LEFT JOIN\n",
    "            (\n",
    "                SELECT\n",
    "                    customer_id,\n",
    "                    date_partition as purchase_dt,\n",
    "                    purchased\n",
    "                FROM product_purchases\n",
    "                WHERE\n",
    "                    product_cd = '{product}'\n",
    "            ) b\n",
    "            ON a.customer_id = b.customer_id and a.next_date_partition = b.purchase_dt\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "    print(f\"Running for '{product}'...\")\n",
    "\n",
    "    # Get dataset\n",
    "    print(\"\\tBuilding dataset...\")\n",
    "    dataset = run_query(engine, sql)\n",
    "    dataset = pd.merge(df, dataset, on=[\"customer_id\", \"date_partition\"])\n",
    "    dataset[\"age\"] = dataset[\"age\"].map(lambda x: int(x) if x != ' NA' else None)\n",
    "    dataset = dataset.drop_duplicates(subset=features+[target])\n",
    "\n",
    "    # Downsample majority class\n",
    "    majority_class = dataset[dataset['target'] == 0]\n",
    "    minority_class = dataset[dataset['target'] == 1]\n",
    "    majority_class_downsampled = majority_class.sample(n=len(minority_class), random_state=SEED)\n",
    "    dataset = pd.concat([majority_class_downsampled, minority_class])\n",
    "    dataset = dataset.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    dataset = dataset.sort_values(by=[\"date_partition\", \"customer_id\"], ascending=True)\n",
    "    \n",
    "    # Spit into X,y pairs\n",
    "    print(\"\\tPreparing X,y...\")\n",
    "    X = dataset[features]\n",
    "    y = dataset[target]\n",
    "\n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=SEED)\n",
    "\n",
    "    # Create transformers for both numerical and categorical columns\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, num_features),\n",
    "            ('cat', categorical_transformer, cat_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Define the full pipeline, including a classifier\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=SEED, class_weight=\"balanced\"))\n",
    "    ])\n",
    "    \n",
    "    # Train model on training set\n",
    "    print(\"\\tTraining model...\")\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    print(\"\\tEvaluation on test set...\")\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    print(f\"\\tF1: {f1_score(y_test, y_pred > 0.5)}\")\n",
    "    print(f\"\\tPrecision: {precision_score(y_test, y_pred > 0.5)}\")\n",
    "    print(f\"\\tRecall: {recall_score(y_test, y_pred > 0.5)}\")\n",
    "    print(f\"\\tAUC: {roc_auc_score(y_test, y_pred)}\")\n",
    "\n",
    "    feature_importances[product] = {key: value for key, value in zip(features, model[-1][1].feature_importances_)}\n",
    "    feature_importances[product] = dict(sorted(feature_importances[product].items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2db40666-d270-45b3-b5b8-1abacc8a650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retail\n",
      "{'new_customer': 0.12332095663724885, 'sex': 0.053538430122325696, 'acquisition_channel': 0.051360467243482345, 'activity_status': 0.02075991478503429, 'education_segment': 0.01637786608781471, 'disability_insurance': 0.014308136730016716, 'sbi': 0.008150557438112499, 'investment_account': 0.0070810070165905825, 'date_month': 0.005774621126807728, 'legal_insurance': 0.0006016321981816715, 'accounting_link': 0.0005197588648432765, 'accounting_package': 0.00045992513365500533, 'job_insurance': 0.0003611998971576515, 'retail': 0.00032977070701463006, 'company_insurance': 0.00029787387340204935, 'travel_insurance': 0.00024680815971793224, 'age': 0.00021302966721686913, 'accident_insurance': 8.336496508225602e-05, 'days_since_last_purchase': 7.740956891571908e-05, 'customer_seniority': 6.960670456719695e-05, 'pension': 4.719410267752999e-05, 'credit_card': 2.9243420217663945e-06, 'gross_income': 0.0, 'times_purchased': 0.0}\n",
      "\n",
      "company_insurance\n",
      "{'new_customer': 0.10794505755467736, 'acquisition_channel': 0.08271263920576975, 'sex': 0.08220644952921992, 'investment_account': 0.032399660146587884, 'activity_status': 0.016031173712169562, 'education_segment': 0.005303616928138995, 'disability_insurance': 0.002979174158300207, 'date_month': 0.0006793996205291927, 'sbi': 0.0005730560281657254, 'legal_insurance': 0.0004846582561163431, 'age': 0.0004663017286107409, 'company_insurance': 0.000360806998075898, 'job_insurance': 0.00033206599049325755, 'accounting_package': 0.00031261940079676197, 'accounting_link': 0.00029289522389618604, 'customer_seniority': 0.00023986341107355211, 'travel_insurance': 0.0002336181726354561, 'retail': 0.00022578528845550966, 'credit_card': 7.403397193900041e-05, 'pension': 0.0, 'accident_insurance': 0.0, 'gross_income': 0.0, 'times_purchased': 0.0, 'days_since_last_purchase': 0.0}\n",
      "\n",
      "accounting_package\n",
      "{'sex': 0.6112995283898516, 'investment_account': 0.05148033458308196, 'sbi': 0.002905407124004208, 'new_customer': 0.0, 'acquisition_channel': 0.0, 'activity_status': 0.0, 'education_segment': 0.0, 'date_month': 0.0, 'disability_insurance': 0.0, 'retail': 0.0, 'accounting_link': 0.0, 'company_insurance': 0.0, 'accounting_package': 0.0, 'pension': 0.0, 'credit_card': 0.0, 'travel_insurance': 0.0, 'job_insurance': 0.0, 'legal_insurance': 0.0, 'accident_insurance': 0.0, 'age': 0.0, 'customer_seniority': 0.0, 'gross_income': 0.0, 'times_purchased': 0.0, 'days_since_last_purchase': 0.0}\n",
      "\n",
      "pension\n",
      "{'sex': 0.07601149275044483, 'acquisition_channel': 0.06344222324046425, 'new_customer': 0.059060825383575905, 'education_segment': 0.05735579230461242, 'activity_status': 0.037497183215714276, 'investment_account': 0.02084582232838998, 'date_month': 0.0078849272253381, 'sbi': 0.007075111698600534, 'disability_insurance': 0.0043664018186760035, 'age': 0.0005328052290175612, 'pension': 0.0004725287491784144, 'gross_income': 0.00027146421667625, 'credit_card': 0.0002563526361538215, 'accounting_link': 0.0002559778701396361, 'legal_insurance': 0.00023197246381658068, 'customer_seniority': 0.00022821195701337548, 'accident_insurance': 0.0002010376041746351, 'company_insurance': 0.00010812750930469265, 'days_since_last_purchase': 7.362812802588637e-05, 'times_purchased': 5.776914460972149e-05, 'travel_insurance': 5.65539295028204e-05, 'job_insurance': 8.226941470572126e-08, 'retail': 0.0, 'accounting_package': 0.0}\n",
      "\n",
      "credit_card\n",
      "{'new_customer': 0.1621429371361772, 'acquisition_channel': 0.07199351724812683, 'sex': 0.06551570038862035, 'education_segment': 0.0468598081816644, 'activity_status': 0.03671652682032086, 'date_month': 0.005958492017018601, 'sbi': 0.0044293926681420745, 'disability_insurance': 0.0018845159012574456, 'investment_account': 0.0015071604749746182, 'legal_insurance': 0.001128608453508787, 'travel_insurance': 0.001039115422409559, 'accounting_link': 0.0006182216048048197, 'accident_insurance': 0.0005925603036293589, 'job_insurance': 0.0005703600360127971, 'accounting_package': 0.0005200222084641783, 'company_insurance': 0.0004958784541874005, 'times_purchased': 0.0003184238134534567, 'age': 0.00028494060355466254, 'retail': 0.00021066552414773348, 'customer_seniority': 0.00018956271572196146, 'credit_card': 0.0001501420172088973, 'gross_income': 7.761744714883924e-05, 'pension': 3.310778143087148e-05, 'days_since_last_purchase': 2.4729072232248963e-05}\n",
      "\n",
      "travel_insurance\n",
      "{'sex': 0.14155294305540994, 'new_customer': 0.09289295110570357, 'acquisition_channel': 0.08118071746876035, 'date_month': 0.013002429342216517, 'sbi': 0.007631425442646556, 'investment_account': 0.004061727852122939, 'disability_insurance': 0.002646250395432711, 'times_purchased': 0.0008598590363876543, 'legal_insurance': 0.000829855075434939, 'job_insurance': 0.000517214775237871, 'accident_insurance': 0.0004346561164293029, 'accounting_package': 0.00042127356447661736, 'accounting_link': 0.00027982777965011255, 'travel_insurance': 9.404512924440808e-05, 'age': 7.929280166415337e-05, 'company_insurance': 2.2649541964089354e-06, 'activity_status': 0.0, 'education_segment': 0.0, 'retail': 0.0, 'pension': 0.0, 'credit_card': 0.0, 'customer_seniority': 0.0, 'gross_income': 0.0, 'days_since_last_purchase': 0.0}\n",
      "\n",
      "job_insurance\n",
      "{'sex': 0.09014063961424981, 'new_customer': 0.08123461142921394, 'acquisition_channel': 0.06840683745991742, 'sbi': 0.017980825967745712, 'activity_status': 0.013992494395803549, 'date_month': 0.011455972283152248, 'accounting_package': 0.00668845791522778, 'disability_insurance': 0.003301982004317847, 'accident_insurance': 0.002668375902594465, 'legal_insurance': 0.0019961223389366894, 'investment_account': 0.0017977554703653181, 'company_insurance': 0.000801190002678161, 'age': 0.0007983267685827368, 'job_insurance': 0.0007522911110218807, 'education_segment': 0.0006586339868408641, 'retail': 0.0005832727139616356, 'customer_seniority': 0.0005832727139616355, 'pension': 4.090920583949894e-06, 'accounting_link': 0.0, 'credit_card': 0.0, 'travel_insurance': 0.0, 'gross_income': 0.0, 'times_purchased': 0.0, 'days_since_last_purchase': 0.0}\n",
      "\n",
      "legal_insurance\n",
      "{'legal_insurance': 0.30227499320221174, 'acquisition_channel': 0.2844106855198117, 'new_customer': 0.04959288450184791, 'travel_insurance': 0.03831986527245666, 'sex': 0.006789502044895808, 'education_segment': 2.2724955221076556e-16, 'activity_status': 0.0, 'sbi': 0.0, 'date_month': 0.0, 'investment_account': 0.0, 'disability_insurance': 0.0, 'retail': 0.0, 'accounting_link': 0.0, 'company_insurance': 0.0, 'accounting_package': 0.0, 'pension': 0.0, 'credit_card': 0.0, 'job_insurance': 0.0, 'accident_insurance': 0.0, 'age': 0.0, 'customer_seniority': 0.0, 'gross_income': 0.0, 'times_purchased': 0.0, 'days_since_last_purchase': 0.0}\n",
      "\n",
      "accident_insurance\n",
      "{'sex': 0.16527777789536105, 'new_customer': 0.0948468718511054, 'disability_insurance': 0.07575355455180842, 'acquisition_channel': 0.0734393877490176, 'sbi': 0.00990323327528812, 'date_month': 0.007843439699426956, 'activity_status': 0.006655356922247255, 'education_segment': 0.0033538497064147556, 'accounting_link': 0.0022712930905443413, 'legal_insurance': 0.0018659563181793926, 'age': 0.001532200153102731, 'accounting_package': 0.001113504010451928, 'company_insurance': 0.0008254525204492347, 'accident_insurance': 0.0008090340607401847, 'travel_insurance': 0.0007206540075106369, 'times_purchased': 0.00037756979927671053, 'job_insurance': 0.00036956074377336273, 'customer_seniority': 0.00034907688235059247, 'credit_card': 0.0001825766018914685, 'retail': 0.000103029525448235, 'pension': 6.108808039905751e-05, 'days_since_last_purchase': 5.369223809494436e-05, 'gross_income': 1.4394675718893254e-06, 'investment_account': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for product in products:\n",
    "    print(product)\n",
    "    print(feature_importances[product])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cfab2-1066-42b7-8068-ab54d24e01c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
